<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>R&D: Render Engine</title>
<link rel="stylesheet" href="../style.css" />
</head>

<header>
  <a href='../index.html'>
    <img src="../icons/logo.png" alt="Website Logo" class="logo">
  </a>

  <a href="https://github.com/AndrewWhatling" target="_blank">
    <img src="../icons/github.svg" alt="Github link" class="github">
  </a>

  <a href="https://www.linkedin.com/in/andrew-whatling-2704a6259" target="_blank">
    <img src="../icons/linkedin.svg" alt="Linkedin link" class="linkedin">
  </a>
  
</header>

<body>
  
  <div class="container portfolio-container">

    <div class="portfolio-text-double box">
      <h2>Helix - Custom CPU Based Render Engine</h2>
    </div>

    <div class="portfolio-photo-hero">
      <img src="../images/renders/hero_render.png" alt="Render" />
    </div>

    <div class="portfolio-text-double box">
      <p>
        <br>
        Helix is a CPU rendering software I started briefly in summer, and since late September 2025, have
        dedicated a lot more time to. There were several reasons as to why I wanted to take on this
        project, I was originally inspired by Sebastian Lague's "Coding Adventure" series, especially
        his raytracer and fluid solver projects.
        <br><br>
      </p>
      <p>
        This project also opened up a variety of learning opportunities and new challenges for me. At the time,
        I had recently gotten a new laptop, and setup Arch Linux for the first time. As well as this, I also started
        using Neovim, a text editor with a lot of keyboard-centric flexibility and a steep learning curve. This
        project gave me the perfect excuse to sink hours into learning, as well as getting comfortable with all of
        these new concepts.
      </p>
      <p>
        <br>
        Now one of the first questions I get when I talk about my render engine is 'Why call it "Helix?"'.
        Whenever I imagine the word helix, the first thing that comes to mind is DNA, or a "double helix".
        This project was my first time using C++, I wanted to challenge myself, and had the drive to create
        this project, so having to learn a new programming language wasn't going to stop me. One of the first
        questions I had when first learning C++ was, "What's a double?", turns out it's a float with double the
        precision, and that's where the idea first came from to call it Helix, and it's stuck ever since.
      </p>
      <p>
        <br>
        Below are my latest renders, as well as the journey that got me here.
      </p>
    </div>
    
    <div class="portfolio-text-double box">
      <h2>Data Passes</h2>
      <h3>
        Current working passes - Cryptomatte object/material, Depth, World space Position, 
        Normals and Camera Facing Ratio.
      </h3>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/data_pass_cryptomatte.png" alt="Cryptomatte" />
      <p>Cryptomatte</p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/data_pass_depth.png" alt="Depth" />
      <p>Depth</p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/data_pass_world_space_position.png" alt="World Space Position" />
      <p>World Space Position</p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/data_pass_normals.png" alt="Normals" />
      <p>Normals</p>
    </div>

    <div class="portfolio-photo-double">
      <img src="../images/renders/data_pass_facing_ratio.png" alt="Camera Facing Ratio" />
      <p>Camera Facing Ratio</p>
    </div>

    <div class="portfolio-text-double box">
      <h2>Depth Of Field</h2>
      <h3>
        Using a thin-lens camera model, default settings for the Black Magic Mini Ursa 4.6K.
      </h3>
    </div>

    <div class="portfolio-photo-double">
      <img src="../images/renders/two_dragons_fov_30_2025_oct_27th_10_38.png" alt="Render" />
    </div>

    <div class="portfolio-text-double box">
      <h2>Material Types</h2>
      <h3>
        Current working Materials - Lambertian (Matte), Metallic, Dielectric (Glass) and Emissive.
      </h3>
    </div>
    
    <div class="portfolio-photo">
      <img src="../images/renders/purple_dragon.png" alt="Render" />
      <p>Dielectric (Glass)</p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/lambert_two_dragons_400_primary_2025_oct_27th_10_37.png" alt="Render" />
      <p>Lambertian (Matte)</p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/metallic_two_dragons_400_primary_2025_oct_27th_11_06.png" alt="Render" />
      <p>Metallic</p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/sphere_light_2025_oct_27th_16_01.png" alt="Render" />
      <p>Emissive</p>
    </div>


    <div class="portfolio-text-double box">
      <h2>The Journey</h2>
    </div>

    <div class="portfolio-text box">
      <p>
        I started my render engine off by initially following along a series called "Raytracing in one weekend." 
        I ran through the initial few chapters of that series. This started with creating very basic images, and 
        exporting through a format called .PPM. After that was adding a basic sphere into the scene, not as geometry,
        but as the mathematical representation. Once that was working, I added basic anti-aliasing and reading of what
        would be the normals of the sphere, and that finally lead to the image you see here.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/antialiasing_2025_sept_30th_12_08.png" alt="Sphere with normals " />
    </div>

    <div class="portfolio-text box">
      <p>
        Once that was complete, the next step was to actually get some form of basic shading. I used a very basic
        shading model to start off with, and the first material type I went with was "Lambertian", a lambert shader, 
        commonly refered to as "Matte shading". This refers to a perfectly diffuse material.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/lambert_shadows_2025_oct_01st_01_41.png" alt="Lambert material" />
    </div>

    <div class="portfolio-text box">
      <p>
        This wasn't without issue however. During this I had a error in my code when it came to the calculation of 
        how ambient light should interact with the sphere's that I had. This resulted in the spheres coming out as 
        a flat grey color with no observable depth to them.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/greyball_no_shadows_2025_oct_01st_01_03.png" alt="Greyballs no depth" />
    </div>

    <div class="portfolio-text box">
      <p>
        The next material type on the list was metals. The shading model I was using took in 2 parameters for metals. 
        First was the color of the metal in RGB, and the second was a 0 to 1 range value called "Fuzz", essentially how 
        blurred the metal's reflection was.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/metal_lambertian_2025_oct_02nd_14_27.png" alt="Metal spheres" />
    </div>

    <div class="portfolio-text box">
      <p>
        After that was programming in a transparent material. The type of transparency I am using is a pure Dielectric model.
        More specifically I am only using Index of Refraction (IOR) to affect the level of transparency that the material has.
        I decided to add a second sphere within the first glass sphere, so that the material can act as though it has thickness,
        rather than just being a solid block of glass.        
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/dialectrics_bubble_2025_oct_05th_22_06.png" alt="Glass bubble sphere" />
    </div>

    <div class="portfolio-text box">
      <p>
        During this stage I also decided to write a system to convert my image from linear colorspace to sRGB, the images just 
        looked really off without having that implemented. Turns out for linear to sRGB it's a single mathematical formula. If 
        you're interested it's here.<br><br>
        
        linear component refers to red green or blue, depending on which is give, and this just checks if that component is less than 
        or equal to 0.0031308, and if it is, then multiply by 12.92, otherwise you take the component, apply an exponent of 1 divide 2.4,
        multiply that by 1.055, and then subtract 0.055 from that result. 
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/engine/linear_to_srgb.png" alt="Linear to sRGB code" />
    </div>

    <div class="portfolio-text box">
      <p>
        Once I had those materials sorted, it was time to make the camera movable. The main challenge with this was that in order 
        to render objects, you first have to convert the world from world space to camera space (space where the camera is at the 
        center of the world), and from there you then convert to 2D pixel space, so applying a transformation to the camera meant 
        adding the world to camera space transformation as well.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/camera_lookat_oct_07th_22_14.png" alt="Camera moved" />
    </div>

    <div class="portfolio-text box">
      <p>
        After making the camera movable, the next thing was to get depth of field working, make the scene feel a bit less CG, as much 
        as you can at this stage anyway. The method used for this was to take the sample position for each pixel, and depending on the 
        field of view given (setting up a camera lens model for DOF will come later), as well as the distance from the focal plane to 
        the current ray of light's length before hitting an object, adds an offset in each direction, thus blurring the image to give 
        depth of field.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/depth_of_field_2025_oct_07th_23_26.png" alt="Depth of Field" />
    </div>

    <div class="portfolio-text box">
      <p>
        And with that, I had finished the "Raytracing in One Weekend" Volume One. I had perfect sphere's rendering in all sorts of 
        basic materials, but there was something (actually a lot of "somethings") missing from the render engine. I had a quick glance 
        through the other books of the same series, but ultimately none of them described how to take the next step that I wanted to
        take, and so it was finally time to drop the guidebook and see what happens. Now the real question, what's the next step?
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/raytracing_in_one_weekend_complete_2025_oct_08th_03_06.png" alt="Raytracing in One Weekend Complete Render" />
    </div>

    <div class="portfolio-text box">
      <p>
        GEOMETRY! Specifically I wanted to get .Obj files loaded into my scene, USD and the rest can come later but for now I just 
        want models in my scene. Now this involved a few different steps before getting it working. Step one, I needed a way to actually 
        read the Obj files into my render engine. Step two, converting those models into something that my render engine can use. 
        Finally step three, using those models to create something in my image, and in this case I went back to square one right at the 
        start of Raytracing in One Weekend, but from the angle of implementing Obj instead of spheres, start in a small capacity and then 
        scale up to production seemed like the best way to handle this challenge.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/obj_loaded_2025_oct_12th_03_28.png" alt="Obj Loaded" />
    </div>

    <div class="portfolio-text box">
      <p>
        For loading the .Obj file, luckily it's essentially just a text file with a specific formatting for vertices, normals, uvs and 
        faces. Through C++ I open the file line by line, and check if the first string of characters before a whitespace is either "v"
        (vertex, the position of the point), "vt" (vertex texture, more commonly it's your UV attribute), "vn" (vertex normal) or "f" 
        (the face the specified vertices make up). As I check through them I store them in a custom class I have setup, which just adds 
        them to specific lists (in C++ this is adding to a "vector", which is a dynamically sized list in C++, but for all intensive 
        purposes I'm going to refer to them as lists, as that will be less confusing for anyone familar with 3D who start reading about
        a vector with over 100 components).<br><br>

        I will get to how the creating faces for the render engine to use works later down the line when I start talking about implementing
        quads into the render engine, but for now, the idea was to only give my render engine triangles to work with, and then I just took 
        the first 3 points and made a triangle out of them using the lists I gathered above.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/engine/load_obj.png" alt="Load Obj Code" />
    </div>

    <div class="portfolio-text box">
      <p>
        Now for the last part of this, which is just figuring out if the triangles I have intersect with the ray of light currently being 
        fired from the camera. At this point I want to quickly cover something, the way light works in a raytracer is a bit strange. Rather 
        than a light firing rays in all directions, praying that one of those rays hits something, we instead shoot rays from the camera, 
        and have them bounce around in the scene until either a set number of bounces is reached, or if a light source is hit. There aren't 
        any light sources aside from just an ambient light in the scene at the moment and so it's just until the bounce limit is reached.<br><br>

        With that out the way, ray intersection of triangles. Luckily for me this has been studied a LOT and so we have a fun little equation 
        for calculating just this, "Barycentric Coordinates". The idea here is we take the 3 points, and then weight their average influence 
        on our triangle, all of which adds up to a total of 1. From here we can calculate that any position within the triangle will have a 
        positive value, and anything outside will have a negative value, and we don't have to worry about any kind of twisting since a triangle 
        always sits on it's given plane, meaning no matter how much you try, you can't twist it over itself, unlike something like a quad.
        Finally we then check if the ray position ever passes through a positive coordinate, and if it does, we have a hit.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/engine/barycentric_coordinates.png" alt="Barycentric Coordinates Code" />
    </div>

    <div class="portfolio-text box">
      <p>
        Next up, I wanted to give my mesh some color, same as before I started with normals rather than a more complex material. The reason 
        for this is so I don't have to worry about calculating how light bounces around my scene and off my geometry, I can just see if it 
        does hit, and if it does then set it to the value of the normals.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/face_normals_2025_oct_13th_02_22.png" alt="Obj Face Normals" />
    </div>

    <div class="portfolio-text box">
      <p>
        Now that I know that's working, I decided it was finally time to start looking at adding an actual material to my mesh. Lucky for 
        me it turns out there really wasn't much difference at all with the math from one to another, it was just a matter of setting up 
        my triangle class so that it could be assigned a material.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/flat_shaded_2025_oct_13th_03_01.png" alt="Obj Flat Shaded" />
    </div>

    <div class="portfolio-text box">
      <p>
        Next was going through and recreating my scene with multiple objects. More specifically I found having a matte groundplane, along 
        with a matte, glass and metallic sphere proved to be quite a nice testing ground for my render engine, at least at this scale anyway.<br><br>

        Now sadly while it may sound like this has been smooth sailing so far, it really wasn't. This render is actually a good example of 
        one of the issues I had, and it steps from the fact that just because my code doesn't error, doesn't mean the math behind it is working
        as it should be. It may be a bit hard to see, but towards all the poles on the geometry (the part of the faces nearest the points), you 
        should be able to see a wierd bit of banding happening, where there's this curved shadow. It's easiest to see at the bottom pole of the 
        center model.<br><br>

        This was caused from something really silly, I wasn't normalising my ray when I fired it from the camera, which I believe meant that 
        lights shooting from the edge of the scene were bouncing around in too large an increment compared to the center pixels, and so they 
        caused uneven bounces, ultimately leading to that artifact there.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/obj_lambertian_metallic_dielectric_2025_oct_15th_04_19.png" alt="Obj Basic Materials" />
    </div>

    <div class="portfolio-text box">
      <p>
        Alright and here's the fixed version, actually if you look between the render here and the one above, you can start playing spot 
        the difference with how many things were actually being affected because of just how long the length of a light ray was.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/shadow_banding_fixed_2025_oct_18th_21_46.png" alt="Shadow Banding Fixed" />
    </div>

    <div class="portfolio-text box">
      <p>
        Alright I'm going to blaze through this next bit as you've seen it already, this time thought I was aiming to get a larger mesh 
        working. My scene at this point was maybe 100 triangles if I am being generous. I wanted to render a chinese dragon model that 
        often gets used for benchmarking and R&D tests, and that full mesh is ~800K polygons, so slightly more that I currently have on 
        hand, but don't worry, light work my render engine's got this! Annnnnnnnd it wouldn't even render a pixel after 10 minutes, nice.<br><br>

        Alright baby steps, I lowered my expectations for the minute and decided to go with a roughly ~800 poly mesh, much lighter but 
        even this was a struggle, and so my next steps was optimisation to try and reach my goal of 1 million polygons. 
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/dragon_low_loaded_2025_oct_12th_04_14.png" alt="Dragon Low Loaded" />
    </div>

    <div class="portfolio-text box">
      <p>
        Now my first optimisation technique I used was called multi-threading. Essentially I was only using one core on my laptop at the 
        time, when it's so much faster to use all of them, in my case I was going from 1 core to 24, so 24 times the number of pixels 
        rendering at one time. Now this doesn't mean that you get 24x the performance, as nice as that would be, it's roughly 6x faster 
        but can be up to 10x depending on what I was doing when rendering. Still not nearly enough for that dragon mesh, great that I can 
        do 24 pixels at once but if I can't even get one to render whats the use putting 24 of them on to not do anything.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/normal_dragon_2025_oct_13th_03_23.png" alt="Dragon Low Normals" />
    </div>

    <div class="portfolio-text box">
      <p>
        As for multi-threading specifically. I'm setting up a threadpool (a class that automatically manages all the threads on my laptop)
        and from there I have different functions that I call at different times in the rendering process. What that means is that when a 
        job starts, my threadpool gets all the workers, and then one by one will asign them a task from that job, and will remove them from 
        the current available pool. Then when a worker finishes their task, they will feed back the result, then clean up any residual data,
        before finally being added back to the active pool and the cycle starts all over again until the job is complete.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/albedo_dragon_low_2025_oct_13th_04_22.png" alt="Dragon Low Shaded" />
    </div>

    <div class="portfolio-text box">
      <p>
        Alright time for some more optimisation. This time I was getting a mesh of roughly 8K polygons working, which meant I needed to finally 
        start looking at a very powerful algorithm for speeding up calculations like these, A Bounding Volume Hierarchy, or BVH for short.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/dragon_mid_2025_oct_25th_03_04.png" alt="Dragon Mid Shaded" />
    </div>

    <div class="portfolio-text box">
      <p>
        The idea behind a BVH is to take a given mesh, create a bounding box from it, and then recursively divide that bounding box until you get 
        a much more reasonable number of boxes to loop through when a ray is fired. Lets say we have 1 million polygons, since that's what I'm aiming 
        for here. Every time a ray is fired from the camera, the render engine has to go through every one of those million polygons, and check if it 
        intersected with that polygon. Then lets say we have 20 samples per pixel, meaning it has to do that 20 times! 20 million processes for a 
        single pixel. A 1920x1080 image is roughly 2 million pixels, so 2 million times 20 million and this is getting into beyond silly teritory.

      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/engine/bvh_code.png" alt="Dragon High Shaded" />
    </div>

    <div class="portfolio-text box">
      <p>
        Instead, with a BVH, we check the bounding box, if it misses, continue, thats already 1 process instead of 1 million, if it hits however,
        we then split the bounding box in half, and check each half, and we repeat this until we get to a recursion depth limit, I set this limit 
        because otherwise my BVH would try to create bounding boxes until it gets down to a single polygon, at which point its another several million 
        calculations to make the BVH in the first place. Once it reaches that recursion limit, it then loops through all the polygons in that bounding 
        region, and then continues the calculations. Essentially you'd be looking at maybe 50-100 checks a sample, which is a much nicer number to put 
        it blunt.<br><br>

        The image to the right here is a Visualisation of BVH construction in Houdini, to the left, is the bounding boxes at roughly depth 10. On the 
        right, you have all the depths up till the same depth overlayed on top of one another, you can see as the depth increases, any bounding boxes 
        that don't have any primitives get removed.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/engine/bvh_vis.png" alt="Bounding Volume Hierarchy Visualisation" />
    </div>

    <div class="portfolio-text box">
      <p>
        With that out the way, it was now time for the big test, could it handle the full resolution dragon. Turns out, yes, yes it could. It took me 
        a little while to realise about the depth recursion limit that I spoke about earlier, so at first it didn't want to play ball, but once I added 
        that, it was happy to start rendering. <br><br>

        There was one other little issue though that cropped up, NaNs! Not the kind that make amazing roast dinners sadly, the kind that break your renders.
        NaNs stands for "not a number" and this issue caused my .PPM file format to break. PPM basically just lists numbers between 0 and 255 representing 
        red, green and blue respectively, the issue that happens with NaNs in this case, instead of 0 to 255, a NaN value defaults to the lowest 32 bit 
        integer number, -2,147,483,648 to be specific, and that in turn makes any pixels after the NaN in my image break and just don't show up. The fix was 
        just adding a failsafe where if the length of the ray vector was too short or not a finite number then set the vector to point directly from the normal.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/dragon_high_NaN_2025_oct_26th_02_20.png" alt="Dragon High Shaded" />
    </div>

    <div class="portfolio-text box">
      <p>
        After that I started having a play trying to get renders working, I now had 2 dragons, so we have easily hit the 1 million mark, and I was very happy 
        at this point. My first test was running with just lambert shaders. The other thing that I did at this point was finally implementing quads.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/lambert_two_dragons_400_primary_2025_oct_27th_10_37.png" alt="Lambert Two Dragons" />
    </div>

    <div class="portfolio-text box">
      <p>
        Quads give me a lot more options for what I can add to my render engine, and I no longer need to triangulate all the meshes coming into the render 
        engine. Now the easiest way I thought of to deal with rendering quads, is rather than triangulating outside of the render engine, why not do that 
        internally during the file reading stage. The idea behind this, I applied procedurally rather than hard coding, which was just looping through each 
        face, and taking point 0, the number of the current loop, and then the number of the current loop + 1. So for example, you would have points 0, 1 
        and 2 making a triangle, and 0, 2 and 3 making another. This also means if I was to load in ngons, it would accept them, but if you had very unstable 
        geometry then there would definitely be artifacting, a future problem but its not a high priority at this moment in time. 
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/engine/quads_implemented.png" alt="Quads Code" />
    </div>


    <div class="portfolio-text box">
      <p>
        After that I tried rendering the dragon pair with field of view. I chucked on 2 metallic shaders with fuzz enabled and this was the result.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/two_dragons_fov_30_2025_oct_27th_10_38.png" alt="Two Dragons Field Of View" />
    </div>

    <div class="portfolio-text box">
      <p>
        Next up, I implemented an emissive texture. The reason for this was actually to use it as a light source, in the long run I plan on playing 
        around with different ideas for lights, but this works at the moment. The main thing that will affect this is implementing more advanced 
        shaders, in particular when I add physically based materials, but that will only be happening after upgrading to a path tracer.<br><br>

        I decided to reuse the mathematical spheres for this light actually, was nice to finally have another use case for them now that I was 
        reading in geometry.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/sphere_light_2025_oct_27th_16_01.png" alt="Sphere Light Dragons" />
    </div>

    <div class="portfolio-text box">
      <p>
        After that, I wanted to see how the light would interact with a metallic shader, and so I chucked on some chrome and set it to render.
        Here's the result of that experiment.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/metallic_two_dragons_400_primary_2025_oct_27th_11_06.png" alt="Metallic Two Dragons" />
    </div>

    <div class="portfolio-text box">
      <p>
        Next, I wanted to use a cornell box, I felt that using something like this makes it a lot easier to compare to other engines and such. 
        This is also a very common model used for benchmarking, along side the dragon I was already using, and so I fired off my first render 
        of it here.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/cornell_dragon_01_2025_oct_27th_23_18.png" alt="Cornell Box Dragon" />
    </div>

    <div class="portfolio-text box">
      <p>
        I then had a fun idea, how about adding an absorption property to the glass. What this means is if the absorption of the glass is set 
        to 0, 1, 0, it will completly absorb the green channel of R,G,B, and then bounces back purple. That's exactly what you see here.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/purple_dragon.png" alt="Purple Dragon" />
    </div>

    <div class="portfolio-text box">
      <p>
        After all of this, now I finally decided that my render engine was at the stage where I would should implement .exr files. And so also 
        came my first use of an external library, the library responsible for exr files. Up until this point, everything I had in my code was 
        written from the ground up, be it the definition of a vec3 (a vector with 3 axis) all the way to my materials and rendering.<br><br>

        The advantage of this now is I was able to add data passes, this one here is a depth. When a ray collides with a mesh it stores the 
        length of that ray, this has a few advantages when it comes to a bit of optimisation, but in this case is acts as my depth pass value.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/data_pass_depth.png" alt="Data Pass Depth" />
    </div>

    <div class="portfolio-text box">
      <p>
        Next up was normals, which luckily are stored in .obj files, but if they aren't stored, then I calculate the face normal and use that 
        which is slightly less accurate if they haven't been stored, but it works for now.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/data_pass_normals.png" alt="Data Pass Normals" />
    </div>

    <div class="portfolio-text box">
      <p>
        Next is world space position, which again is just sampled from the ray when it hits the mesh.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/data_pass_world_space_position.png" alt="Data Pass World Space Position" />
    </div>

    <div class="portfolio-text box">
      <p>
        Now for one of my favourite data passes: Camera facing ratio. The way this works is by calculating the direction of the normal, which in 
        turn is the direction the face is well, facing. Then we use a dot product to calculate if the ray and the normal are facing towards each 
        other, and based on that we set the value for the facing ratio, a dot product of -1 means the ray and normals directly face each other, 
        and I set a value of 1 in the red channel. If the dot product gives 0, I return 0.5 in the red channel, and anything further than that 
        is on the other side of the mesh and so you can't see it. The reason I return 0.5 instead of 0, is because 0 is the blank space where no 
        geometry is.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/data_pass_facing_ratio.png" alt="Data Pass Facing Ratio" />
    </div>

    <div class="portfolio-text box">
      <p>
        Finally we got onto cryptomatte. The issue with this is that it requires very specific metadata. So not only do I now have IDs for my 
        materials and meshes in my scene that then gets set to the cryptoid, but we also need encryption among other things.  
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/renders/data_pass_cryptomatte.png" alt="Data Pass Cryptomatte" />
    </div>

    <div class="portfolio-text box">
      <p>
        Now for the encryption. Luckily I can use Nuke to read the metadata from exr files rendered out of Houdini with cryptomatte enabled.
        What this lead me to was that the encryption method used was something called murmur_hash3, and luckily enough again, wikipedia 
        happened to have a C based language implementation of this encryption. Then I just needed to setup writing it out alongside the rest 
        of my data passes, which in turn did make me have to overhaul my entire way I was writing out exr files, but in the end it worked and 
        now I had working cryptomattes.
      </p>
    </div>

    <div class="portfolio-photo">
      <img src="../images/engine/murmur_hash3.png" alt="MurMur Hash3 Code" />
    </div>


    <div class="portfolio-text-double box">
      <p>
        And with that, you've reached where I am currently at with my render engine.<br><br>
        
        Thank you for reading and if you have any suggestions for improvements, or any questions about my work, please reach out to be via 
        the email below!<br><br>

        Now if you're interested, the next plans for this project are roughly as follows:<br><br>

        Work out how to do scene serialisation: I am creating nodes in Houdini that will write out to a .json file that my renderer will read in 
        and convert into a scene. In this way I can do lookdev inside of Houdini, and there's potential here to explore GLSL realtime shaders.<br><br>

        After that is converting my raytracer to a path-tracer, and then I will start working on physically based materials.<br><br>
        
        From there I want to try playing around with some more abstract things, there's a lot of optimisations and small quality of life features that
         I will implement in between this when I don't quite have time for the larger elements, but a big one I've been picturing was trying to implement 
         spectral rendering, using wavelengths rather than RGB values so that I can get iridescent textures among other things.<br><br>
        
        There's also a world where I span this off in the direction of interstellar, probably as a side module or a seperate project in hindsight, but 
        creating a render engine that can bend light around a black hole does sound very fun.
      </p>
    </div>

    <div style="height: 1px;"></div>
  </div>
  


</body>

</html>
